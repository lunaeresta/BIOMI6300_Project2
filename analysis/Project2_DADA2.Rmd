---
title: "DADA2 Workflow for BIOMI6300 Project 2"
author: "Luna Eresta Jaya"
date: "2023-04-26"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Loading packages
```{r load}
# Set working directory
setwd("/workdir/lej52/BIOMI6300_Project2_Amplicon_Analysis")

# Load packman packages
# install.packages("pacman")
# Download Software 
pacman::p_load(dada2, tidyverse, patchwork, phyloseq, Biostrings, install = FALSE)
# Ensure correct package
packageVersion("dada2")

# Load in the functions file from code directory
source("/local/workdir/lej52/BIOMI6300_Project2_Amplicon_Analysis/code/functions.R")
```

# Set the path to the seq files
```{r}
# Set path to the gzipped files
path <- "data/sequencing_3"
path

# What files do we have?
list.files(path)

# Use 4 samples first to test workflow
samples <- c("JD-11", "JD-217", "JD-111", "JD-110")
```

# Load in Forward and Reverse reads and assess the quality
```{r}
# Load reads
# Create variable for the forward and the reverse reads

# Forward read variable
forward_reads <- sort(list.files(path, pattern="_1.fastq",
                      full.names=TRUE))

# Reverse read variable
reverse_reads <- sort(list.files(path, pattern="_2.fastq",
                      full.names=TRUE))

# 3. Place filtered files into filtered/subdirectory
# Create a variable holding file names for the Forward and Reverse filtered reads
filtered_forward_reads <- file.path(path, "filtered", paste0(samples, "_R1_filtered.fastq.gz"))
filtered_reverse_reads <- file.path(path, "filtered", paste0(samples, "_R2_filtered.fastq.gz"))

# Show the quality of each base on the reads of first 4 samples
forwardQual4_plot <- plotQualityProfile(forward_reads[1:4])
reverseQual4_plot <- plotQualityProfile(reverse_reads[1:4])

# Plot forward and reverse 4 quality plots together
forwardQual4_plot+reverseQual4_plot
```

# Filtering and trimming
```{r}
filtered_out <- filterAndTrim(forward_reads, filtered_forward_reads,
                              reverse_reads, filtered_reverse_reads,
                              truncLen = c(100, 100), trimLeft = c(20,20),
                              maxN = 0, maxEE = c(1,1), truncQ = 2,
                              rm.phix = TRUE, compress = TRUE,
                              multithread = TRUE)

# Use figaro by Zymo-Research (on github) to trim auotomatically


# Show the quality of each base on the reads of first sample
filtered_forwardQual4_plot <- plotQualityProfile(filtered_forward_reads[1:4])
filtered_reverseQual4_plot <- plotQualityProfile(filtered_reverse_reads[1:4])
filtered_forwardQual4_plot+filtered_reverseQual4_plot

```

# Generate an error model
```{r}
# Learn errors
err_forward_reads <- learnErrors(filtered_forward_reads, multithread = TRUE)
err_reverse_reads <- learnErrors(filtered_reverse_reads, multithread = TRUE)

# Plot the errors
plotErrors(err_forward_reads, nominalQ = TRUE)
plotErrors(err_reverse_reads, nominalQ = TRUE)
```

# Inferring ASVs on the forward and reverse sequences
```{r infer-ASVs}
# run dada2 on the forward seqs
dada_forward <- dada(filtered_forward_reads, err = err_forward_reads, multithread = TRUE)

# run dada2 on the reverse sequences
dada_reverse <- dada(filtered_reverse_reads, err = err_reverse_reads, multithread = TRUE)
dada_reverse[1]
```

# Merge forward and reverse ASVs
```{r merge-FandR-ASVs}
# Merge and forward ASVs and the reverse ASVs
merged_amplicons <- mergePairs(dada_forward, filtered_forward_reads,
                              dada_reverse, filtered_reverse_reads,
                              verbose = TRUE)

# Evaluate the output
merged_amplicons
typeof(merged_amplicons)
length(merged_amplicons)
names(merged_amplicons)

merged_amplicons[1]
```

# Generate a count table
```{r gen-countTable-seqTab}
seqtab <- makeSequenceTable(merged_amplicons)
class(seqtab)
typeof(seqtab)
dim(seqtab)
View(seqtab)

# Inspect the distribution of sequence lengths of all ASVs in the dataset
table(nchar(getSequences(seqtab)))
# why are there samples with more or less than 214 base pairs?
```

I have `r ncol(seqtab)` ASVs in the dataset!

# Check & Remove for Chimeras (Bimeras)
```{r}
# Identify and remove chimeras
seqtab_nochim <- removeBimeraDenovo(seqtab, verbose=TRUE)

# What proportion of counts were removed?
chim_check <- sum(seqtab_nochim)/sum(seqtab) # 0.9725445
frac_removed <- (1-chim_check)*100 #percent
frac_removed
#11.73% percent was removed based on 4 sequencing data files
```

Chimeras represented `r frac_removed` percent of the data.

# Track the sequences through the pipeline
```{r}
# Create a little function to identify number seqs
getN <- function(x) sum(getUniques(x))

# Make the table to track the seqs
track <- cbind(filtered_out,
               sapply(dada_forward, getN),
               sapply(dada_reverse, getN),
               sapply(merged_amplicons, getN),
               rowSums(seqtab_nochim))
head(track)

# Change column names
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- samples
head(track)

# Generate a plot to track the reads through our DADA2 pipeline
track %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var= "names") %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  make_MA_metadata() %>%
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y=num_reads, fill = read_type)) +
  facet_grid(~fraction) +
  geom_line(aes(group = names), color = "grey") +
  geom_point(shape = 21, size = 3, alpha = 0.8) +
  scale_fill_brewer(palette = "Spectral") +
  theme_bw() +
  labs(x = "Filtering Step", y = "Number of Sequences") +
  theme(legend.position = "bottom", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

# First, let's source the colors_and_shapes.R
# This file is in /workdir/in_class_data/colors_and_shapes.R
source("code/colors_and_shapes.R")


# Plot the percent reads retained 
# track %>%
#   as.data.frame() %>%
#   mutate(percent_reads_retained = round((nochim/input)*100, digits = 2)) %>%
#   rownames_to_column(var = "names") %>% # As input to make_MA_metadata
#   make_MA_metadata() %>%
#   # Make the plot! 
#   ggplot(aes(x = fraction, y = percent_reads_retained, fill = fraction)) + 
#   geom_jitter(shape = 21, size = 3, alpha = 0.8) + 
#   geom_boxplot(alpha = 0.4, outlier.shape = NA) + 
#   # scale_fill_manual(values = fraction_colors) + 
#   theme_bw() + 
#   labs(y = "Percent Sequences Retained by DADA2") +
#   theme(axis.title.x = element_blank())
```

# Assign taxonomy
```{r assign-tax}
# The next line took 2 mins to run
taxa <- assignTaxonomy(seqtab_nochim, "/workdir/in_class_data/taxonomy/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)

# the next line took 3 minutes 
taxa <- addSpecies(taxa, "/workdir/in_class_data/taxonomy/silva_species_assignment_v138.1.fa.gz")

# Inspect the taxonomy 
taxa_print <- taxa # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
View(taxa_print)
```


NOTE:
THERE WAS NO MOCK COMMUNITY IN THIS ANALYSIS, HENCE THE CODE CHUNK FOR MEASURING MOCK COMMUNITY DETECTION ACCURACY WAS REMOVED.




# Prepare the data for export!
### 1. ASV Table
```{r prepare-ASV-table}
# Prep the asv table!
samples_out <- rownames(seqtab_nochim)

# Pull out sample names from the fastq file name
sample_names_reformatted <- sapply(strsplit(samples_out, split="_"), `[`, 1)

# Replace the names in our seqtable
rownames(seqtab_nochim) <- sample_names_reformatted

### intuition check
stopifnot(rownames(seqtab_nochim) == sample_names_reformatted)

########### Modify the ASV names and then save a fasta file!
# Give headers more manageable names
# First pull the ASV sequences
asv_seqs <- colnames(seqtab_nochim)

# make headers for oru ASV seq fasta file, which will be our asv names
asv_headers <- vector(dim(seqtab_nochim)[2], mode = "character")

# loop through vector and fill it in with ASV names
for(i in 1:dim(seqtab_nochim)[2]){
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# intuition check
asv_headers


##### Rename ASVs in table then write out our ASV fasta file!
View(seqtab_nochim)
asv_tab <- t(seqtab_nochim)
View(asv_tab)

## Rename our asvs!
row.names(asv_tab) <- sub(">", "", asv_headers)
View(asv_tab)

# Write the count table to a file!
write.table(asv_tab, "data/ASV_counts.tsv", sep="\t", quote=FALSE, col.names=NA)

# Write out the fasta file for reference later on for what seq matches what ASV
asv_fasta <- c(rbind(asv_headers, asv_seqs))

# Save to a file
write(asv_fasta, "data/ASVs.fasta")
```

### 2. Taxonomy Table
```{r prepare-tax-table}
View(taxa)

##### Prepare tax table
# Add the ASV sequences from the rownames to a column
new_tax_tab <- taxa %>%
  as.data.frame() %>%
  rownames_to_column(var = "ASVseqs")

head(new_tax_tab)

# intuition check
stopifnot(new_tax_tab$ASVseqs == colnames(seqtab_nochim))

# Now let's add the ASV names
rownames(new_tax_tab) <- rownames(asv_tab)
View(new_tax_tab)


### Final prep of tax table. Add new column with ASV names
asv_tax <- new_tax_tab %>%
  # add rownames from the count table for easier phyloseq handoff
  mutate(ASV = rownames(asv_tab)) %>%
  # Re-sort the columns with select
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species, ASV, ASVseqs)

View(asv_tax)

# Intuition check
stopifnot(asv_tax$ASV == rownames(asv_tax), rownames(asv_tax) == rownames(asv_tab))

# Write the table
write.table(asv_tax, "data/ASV_taxonomy.tsv", sep="\t", quote=FALSE, col.names=NA)
```

### 3. Metadata
```{r metadata-prep}
# Read in metadata
metadata <- read.csv("data/metadata.txt") %>%
  filter(Sample.Name %in% c("JD-11", "JD-217", "JD-111", "JD-110")) %>%
  dplyr::rename(names = Sample.Name)

head(metadata)

str(metadata)

# Add names to rownames for phyloseq happiness
rownames(metadata) <- metadata$names

# View(metadata)
```
